# Environment parameters

env: BECEC
random_seed: 8888
num_agents: 14

load_param_while_training: True

# Training parameters

model: d4pg
batch_size: 512
num_steps_train: 2_000_000 # number of episodes from all agents
max_ep_length: 1000 # maximum number of steps per episode
replay_mem_size: 5_000_000 # maximum capacity of replay memory
priority_alpha: 0.6 # controls the randomness vs prioritisation of the prioritised sampling (0.0 = Uniform sampling, 1.0 = Greedy prioritisation)
priority_beta_start: 0.4 # starting value of beta - controls to what degree IS weights influence the gradient updates to correct for the bias introduces by priority sampling (0 - no correction, 1 - full correction)
priority_beta_end: 1 # beta will be linearly annelaed from its start value to this value thoughout training
discount_rate: 0.9 # Discount rate (gamma) for future rewards
n_step_returns: 1 # number of future steps to collect experiences for N-step returns
update_agent_ep: 1 # agent gets latest parameters from learner every update_agent_ep episodes
replay_queue_size: 1024 # queue with replays from all the agents
batch_queue_size: 64 # queue with batches given to learner
replay_memory_prioritized: 1
num_episode_save: 100
device: cuda:0
agent_device: cuda:1
save_buffer_on_disk: 0
save_reward_threshold: 1 # difference in best reward to save agent model

# Network parameters

critic_learning_rate: 0.0005
actor_learning_rate: 0.0001
dense_size: 4000 # size of the 2 hidden layers in networks
num_atoms: 11 # number of atoms in output layer of distributed critic
v_min: -500. # lower bound of critic value output distribution (depending on reward and discount_rate)
v_max: 4000. # upper bound of critic value output distribution
tau: 0.005 # parameter for soft target network updates

is_log_max_min_Q: True  # help to log v_min and v_max through the training process
                        # when the critic loss is getting to 0, these values are valid

# Action Noise Type
# type 0: OUNoise
# type 1: Epsilon Greedy (All n_tasks choices as a group)
# type 2: Epsilon Greedy (Every task's choice is independently)
noise_type: 2

# Noise Param
decay_period: 100_000
min_sigma: 0.1
max_sigma: 0.6

# Wolpertinger Architecture, refer to https://arxiv.org/pdf/1512.07679.pdf
# Only can be used in d4pg:
# 1. learner_w_queue use [actor_params, critic_params] instead of just actor_params
# 2. cannot be used with discrete softmax option (action_mode = 1)
# use_wolp: False
# wolp_train_mode: False # train critic network by wolp
# mode 0: Don't use wolp
# mode 1: Use full wolp
# mode 2: Only use wolp in exploration and exploitation (no train)
# mode 3: Only use wolp in exploitation (no train)
wolp_mode: 0
k_nearest_neighbors: 1000 # the k of K-means
points_in_each_axis: 2 # total points number = points_in_each_axis ** dims

# Miscellaneous

results_path: results


# BECEC parameters

# shuffle bs sort in states
# 0: not shuffle
# 1: only shuffle BS choices [0, M-1]
# 2: shuffle BS and Null choices [0, M]
shuffle_bs: 0

sort_tasks: True

M: 10 # number of BSs
delta_t: 10 # number of future slots the algorithm use to allocate at once
T: 200 # number of time slots
# frame: 10 # the length of a frame (warning: was discarded)
n_tasks: 5 # number of tasks in a group
# frame_mode
# mode 0: set a solid frame length
#         can be used in D4PG only when n_tasks is big enough! (Lax implementation)
# mode 1: when number of tasks in cache reaches n_tasks, generate a new frame
frame_mode: 1
# state
# mode 0: observation = BS Info (delta_t slots of C & p) + Task Info (w, alpha, u_0)
# mode 1: observation = BS Info (delta_t slots of C) cast to 2 dimensions + Task Info (w, alpha, u_0)
# mode 2: observation = BS Info (delta_t slots of C) + Task Info (w, alpha, u_0)
state_mode: 1
# action
# plan 0: action = target BS numbers -- for each task, output [-1, 1] and quantify to  M+1 number (M BSs + 1 null)
# plan 1: action = one-hot -- for each task, output (M+1)*[0, 1] as a group (softmax), indicates the one-hot rate of BSs
action_mode: 1
# penalty
# mode 0: throw all external_tasks in the busy BS, utinity += penalty*len(external_tasks)
# mode 1: throw all external_tasks in the busy BS, utinity += penalty*len(exceeded_external_tasks)
# mode 2: throw the last input task one by one, utinity += penalty*len(thrown_external_tasks)
# === belows are without thrown penalty ===
# mode 3: don't penalty thrown tasks, and don't execute allocation of tasks with u-c<0
#         whether penaltying null BS depends on subsquent flag 'is_null_penalty'
# mode 4: don't penalty thrown tasks, penalize u-c<0 but don't execute it
# mode 5: don't penalty thrown tasks
penalty_mode: 5
penalty: -1000.
is_null_penalty: False # whether the null selection should be penaltied

use_entropy: False  # whether using BS selection entropy in the reward
entropy_factor: 500.   # reward + entropy_factor * entropy

stage2_alg_choice: 0 # 二阶段算法 0 是用贪心, 1 是用 dp

test_global_greedy: False # 如果不使用 global, 则会使用第二阶段的算法