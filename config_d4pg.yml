# Environment parameters

env: BECEC
random_seed: 2077
num_agents: 8


# Training parameters

model: d4pg
batch_size: 256
num_steps_train: 1_000_000 # number of episodes from all agents
max_ep_length: 1000 # maximum number of steps per episode
replay_mem_size: 1_000_000 # maximum capacity of replay memory
priority_alpha: 0.6 # controls the randomness vs prioritisation of the prioritised sampling (0.0 = Uniform sampling, 1.0 = Greedy prioritisation)
priority_beta_start: 0.4 # starting value of beta - controls to what degree IS weights influence the gradient updates to correct for the bias introduces by priority sampling (0 - no correction, 1 - full correction)
priority_beta_end: 1 # beta will be linearly annelaed from its start value to this value thoughout training
discount_rate: 0.95 # Discount rate (gamma) for future rewards
n_step_returns: 1 # number of future steps to collect experiences for N-step returns
update_agent_ep: 1 # agent gets latest parameters from learner every update_agent_ep episodes
replay_queue_size: 1024 # queue with replays from all the agents
batch_queue_size: 64 # queue with batches given to learner
replay_memory_prioritized: 1
num_episode_save: 100
device: cuda
agent_device: cuda
save_buffer_on_disk: 0
save_reward_threshold: 1 # difference in best reward to save agent model

max_sigma: 0.4
min_sigma: 0.2

decay_period: 100

# Network parameters

critic_learning_rate: 0.0005
actor_learning_rate: 0.0001
dense_size: 1024 # size of the 2 hidden layers in networks
num_atoms: 51 # number of atoms in output layer of distributed critic
v_min: -3000 # lower bound of critic value output distribution
v_max: 3000 # upper bound of critic value output distribution
tau: 0.005 # parameter for soft target network updates


# Wolpertinger Architecture, refer to https://arxiv.org/pdf/1512.07679.pdf
# Only can be used in d4pg:
# 1. learner_w_queue use [actor_params, critic_params] instead of just actor_params
# 2. 
use_wolp: True
k_nearest_neighbors: 1000 # the k of K-means


# Miscellaneous

results_path: results


# BECEC parameters

# shuffle bs sort in states
# 0: not shuffle
# 1: only shuffle BS choices [0, M-1]
# 2: shuffle BS and Null choices [0, M]
shuffle_bs: 1

M: 20 # number of BSs
delta_t: 30 # number of future slots the algorithm use to allocate at once
T: 230 # number of time slots
frame: 10 # the length of a frame (warning: will be discarded)
n_tasks: 10 # number of tasks in a group
# frame_mode
# mode 0: set a solid frame length
#         can be used in D4PG only when n_tasks is big enough! (Lax implementation)
# mode 1: when number of tasks in cache reaches n_tasks, generate a new frame
frame_mode: 1
# state
# mode 0: observation = BS Info (delta_t slots of C & p) + Task Info (w, alpha)
# mode 1: observation = BS Info (delta_t slots of C) cast to 2 dimensions + Task Info (w, alpha)
# mode 2: observation = BS Info (delta_t slots of C) + Task Info (w, alpha)
state_mode: 1
# action
# plan 0: action = target BS numbers -- for each task, output [-1, 1] and quantify to  M+1 number (M BSs + 1 null)
# plan 1: action = one-hot -- for each task, output (M+1)*[-1, 1] as a group, indicates the one-hot rate of BSs
action_mode: 0
# penalty
# mode 0: throw all external_tasks in the busy BS, utinity += penalty*len(external_tasks)
# mode 1: throw all external_tasks in the busy BS, utinity += penalty*len(exceeded_external_tasks)
# mode 2: throw the last input task one by one, utinity += penalty*len(thrown_external_tasks)
# mode 3: don't penalty thrown tasks, and don't execute allocation of tasks with u-c<0
#         whether penaltying null BS depends on subsquent flag 'is_null_penalty'
penalty_mode: 3
penalty: -1000.
is_null_penalty: False # whether the null selection should be penaltied

use_entropy: True  # whether using BS selection entropy in the reward
entropy_factor: 500.   # reward + entropy_factor * entropy