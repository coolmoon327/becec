# Environment parameters

env: BECEC
random_seed: 2021
num_agents: 1


# Training parameters

model: d3pg
batch_size: 512
num_steps_train: 1_000_000 # number of episodes from all agents
max_ep_length: 1000 # maximum number of steps per episode
replay_mem_size: 1_000_000 # maximum capacity of replay memory
discount_rate: 0.99 # Discount rate (gamma) for future rewards
n_step_returns: 1 # number of future steps to collect experiences for N-step returns
update_agent_ep: 1 # agent gets latest parameters from learner every update_agent_ep episodes
replay_queue_size: 1_024 # queue with replays from all the agents
batch_queue_size: 64 # queue with batches given to learner
num_episode_save: 100
# device: cuda
device: cpu
agent_device: cpu
save_reward_threshold: 1 # difference in best reward to save agent model

decay_period: 100

# Network parameters

critic_learning_rate: 0.0005
actor_learning_rate: 0.00001
dense_size: 256 # size of the 2 hidden layers in networks
tau: 0.005 # parameter for soft target network updates


# Miscellaneous

results_path: results


# BECEC parameters

M: 20 # number of BSs
delta_t: 30 # number of future slots the algorithm use to allocate at once
T: 130 # number of time slots
frame: 10 # the length of a frame (warning: will be discarded)
n_tasks: 10 # number of tasks in a group
# frame_mode
# mode 0: set a solid frame length
#         can be used in D4PG only when n_tasks is big enough! (Lax implementation)
# mode 1: when number of tasks in cache reaches n_tasks, generate a new frame
frame_mode: 1
# state
# mode 0: observation = BS Info (delta_t slots of C & p) + Task Info (w, u_0, alpha)
# mode 1: observation = BS Info (delta_t slots of C & p) cast to 2 dimensions) + Task Info (w, u_0, alpha)
# mode 2: observation = BS Info (delta_t slots of C) + Task Info (w, u_0, alpha)
state_mode: 2
# action
# plan 0: action = target BS numbers -- for each task, output [-1, 1] and quantify to  M+1 number (M BSs + 1 null)
# plan 1: action = one-hot -- for each task, output (M+1)*[-1, 1] as a group, indicates the one-hot rate of BSs
action_mode: 0
# penalty
# mode 0: throw all external_tasks in the busy BS, utinity += penalty*len(external_tasks)
# mode 1: throw all external_tasks in the busy BS, utinity += penalty*len(exceeded_external_tasks)
# mode 2: throw the last input task one by one, utinity += penalty*len(thrown_external_tasks)
penalty_mode: 2
penalty: -1000.